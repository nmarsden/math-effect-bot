<!DOCTYPE html>
<html lang="en-US">
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>Train Brain Experiment - 1c</title>


    <link rel="stylesheet" href="/bootstrap/css/bootstrap.min.css" type="text/css" media="screen" >

    <link href="font-awesome-4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link href="css/tdGame.css" rel="stylesheet">

</head>
<body>

<script type="text/javascript" src="/js/jquery-1.8.2.min.js"></script>
<script type="text/javascript" src="/bootstrap/js/bootstrap.min.js"></script>

<script type="text/javascript" src="/js/td/convnet.js"></script>
<script type="text/javascript" src="/js/td/util.js"></script>
<script type="text/javascript" src="/js/td/deepqlearn.js"></script>

<div class="container main">
    <div class="row">
        <div class="col-md-8">
            <div class="main-content-well well well-small ">
                
<div class="hero-unit">
    <h1>Train Brain Experiment #1</h1>

    <h2>Train the brain to select a player unit that exists</h2>
    <h3>d) Using 1 input with range 1-2 and 1 output in the range 0-7</h3>
    <ul>
        <li>1 input in the range 1-2 to represent the number of player units</li>
        <li>1 output in the range 0-7</li>
        <li>reward brain for outputting an action for a an existing player</li>
        <li>punish brain for outputting an action for a non existing player</li>
        <li>track reward</li>
        <li>reward going consistently up indicates the brain as learned to select an existing player</li>
    </ul>
    eg. For an input of 1,
    <ul>
        <li>reward +1 to the brain for an output in the range 0-3</li>
        <li>reward -2 to the brain for an output in the range 4-7</li>
    </ul>
</div>

            </div>
        </div>
    </div>
</div>

<script>
    function rand(min, max) {
        return Math.floor(Math.random()*(max-min+1)+min);
    }
    var NUM_PLAYER_UNITS = 2;
    var NUM_TURNS = 120000;
    var AVERAGE_WINDOW_SIZE = 400;
    var REWARD_GOOD_ACTION = 1;
    var PUNISH_BAD_ACTION = -2;


    // Initialize brain if necessary
    var num_inputs = 1;   // 1 input in the range 1-10 to represent the number of player units
    var num_actions = 4 * NUM_PLAYER_UNITS;  // 1 output in the range 0-39

//    var num_inputs = 81; // (9 x 9) inputs for board state, each in range 0-11
//    var num_actions = 80; // a number in the range 0-79 : that is 4 possible actions for one of 20 possible players

    var temporal_window = 1; // amount of temporal memory. 0 = agent lives in-the-moment :)
    var network_size = num_inputs * temporal_window + num_actions * temporal_window + num_inputs;

    // the value function network computes a value of taking any of the possible actions
    // given an input state. Here we specify one explicitly the hard way
    // but user could also equivalently instead use opt.hidden_layer_sizes = [20,20]
    // to just insert simple relu hidden layers.
    var layer_defs = [];
    layer_defs.push({type: 'input', out_sx: 1, out_sy: 1, out_depth: network_size});
    layer_defs.push({type: 'fc', num_neurons: 50, activation: 'relu'});
    layer_defs.push({type: 'fc', num_neurons: 50, activation: 'relu'});
    layer_defs.push({type: 'regression', num_neurons: num_actions});

    // options for the Temporal Difference learner that trains the above net
    // by backpropping the temporal difference learning rule.
    var tdtrainer_options = {learning_rate: 0.001, momentum: 0.0, batch_size: 64, l2_decay: 0.01};

    var opt = {};
    opt.temporal_window = temporal_window;
    opt.experience_size = 30000;
    opt.start_learn_threshold = 1000;
    opt.gamma = 0.7;
    opt.learning_steps_total = 200000;
    opt.learning_steps_burnin = 3000;
    opt.epsilon_min = 0.05;
    opt.epsilon_test_time = 0.05;
    opt.layer_defs = layer_defs;
    opt.tdtrainer_options = tdtrainer_options;

    var brain = new deepqlearn.Brain(num_inputs, num_actions, opt); // woohoo

    var array_with_num_inputs_numbers = new Array(1);
    var isValidAction;
    var action;
    var reward;
    var turn;
    //var trackedRewards = [];
    var rewardsSum = 0;
    var rewardsOutput;
    var averageReward;
    var trackAverageRewards = [];


    window.performance.mark('mark_start_learning');


    console.log("Attempt # 27");
    console.log("---------------------------------------------------------");
    console.log("*** Started Learning using " + NUM_TURNS + " iterations ***");
    console.log("---------------------------------------------------------");

    for (turn=0; turn<NUM_TURNS; turn++) {
        // generate random input in range 1-10
        array_with_num_inputs_numbers[0] = rand(1, NUM_PLAYER_UNITS);

        // Get action from brain
        action = brain.forward(array_with_num_inputs_numbers);

        // check if action is valid for input
        isValidAction = action < (4 * array_with_num_inputs_numbers[0]);

        reward = isValidAction ? REWARD_GOOD_ACTION : PUNISH_BAD_ACTION;

        // track reward
        //trackedRewards.push(reward);
        rewardsSum += reward;

        brain.backward(reward); // <-- learning magic happens here

        if (turn % AVERAGE_WINDOW_SIZE == 0) {
            // Track average reward for AVERAGE_WINDOW_SIZE iterations
            averageReward = rewardsSum / AVERAGE_WINDOW_SIZE;
            //averageReward = trackedRewards.reduce(function(a, b) { return a + b }) / AVERAGE_WINDOW_SIZE;
            // averageReward = trackedRewards.slice(-AVERAGE_WINDOW_SIZE).reduce(function(a, b) { return a + b }) / AVERAGE_WINDOW_SIZE;

            trackAverageRewards.push(averageReward);

            // Reset trackedRewards
            //trackedRewards = [];
            rewardsSum = 0;

            // output progress
            console.log("# iterations done: " + turn + ", average Reward: " + averageReward);
        }
    }

    window.performance.mark('mark_stop_learning');
    window.performance.measure('measure_learning_time', 'mark_start_learning', 'mark_stop_learning');


    //console.timeEnd(TIMER_NAME);

    // output reward values
//    console.log("---------------------------------");
//    console.log("*** Rewards for " + NUM_TURNS + " iterations ***");
//    console.log("---------------------------------");
//    rewardsOutput = "turn, reward\n";
//    for (turn=0; turn<NUM_TURNS; turn++) {
//        rewardsOutput += (turn+1) + "," + trackedRewards[turn] + "\n";
//    }
//    console.log(rewardsOutput);

    // output average reward values
    console.log("-------------------------------------------------------------------------------------");
    console.log("*** Average reward every " + AVERAGE_WINDOW_SIZE + " iterations for a total of " + NUM_TURNS + " iterations ***");
    console.log("-------------------------------------------------------------------------------------");
    rewardsOutput = "iterations, average_reward\n";
    for (turn=0; turn<trackAverageRewards.length; turn++) {
        rewardsOutput += ((turn+1)*AVERAGE_WINDOW_SIZE) + "," + trackAverageRewards[turn] + "\n";
    }
    console.log(rewardsOutput);

    var learningTime = window.performance.getEntriesByType('measure');
    console.log("-------------------------------------------------------------------------------------");
    console.log("Learning Time (msecs) :" + learningTime[0].duration);
    console.log("-------------------------------------------------------------------------------------");

</script>
</body>

</html>