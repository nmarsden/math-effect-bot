----------------------------
--- Original source code ---
----------------------------
Math Effect source code on github
- https://github.com/ilfate/laravel.ilfate.net

-----------
--- DOM ---
-----------
DOM for player and one button
-----------------------------
<div class="unit-1 playerUnit">
  <div class="tdButton fa-arrow-circle-down"></div>
</div>

DOM for final modal dialog
--------------------------
<div id="myModal"
    <div id="turnsSurvived">10</div>
    <div id="unitsKilled">2</div>
    <div id="pointsEarned">16</div>

     <a class="btn">Restart</a>
</div>
	 
---------------------
--- Code Snippets ---
---------------------
Click first player button found
-------------------------------
$(".tdButton:visible").first().click()

Output final result
-------------------
console.log("[turns: " + $("#turnsSurvived").text() + "][kills: " + $("#unitsKilled").text() + "][points: " + $("#pointsEarned").text() + "]");

// [turns: 5][kills: 2][points: 3]

AutoPlay game forever
---------------------
var autoPlay = function () {
    if ($("#myModal:visible").size() == 0) {
        $(".tdButton:visible").first().click();
		setTimeout(autoPlay, 1000);
    } else {
        console.log("[turns: " + $("#turnsSurvived").text() + "][kills: " + $("#unitsKilled").text() + "][points: " + $("#pointsEarned").text() + "]");
		
		window.location = "http://ilfate.net/MathEffect"
		setTimeout(autoPlay, 3000);
	}
}

setTimeout(autoPlay, 1000);

-----------------------
--- Q-Learner Notes ---
-----------------------

ConvnetJS - Reinforcement Learning Example
------------------------------------------
http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html

Questions
- What actions should be rewarded/punished?
  a) +5 : reward for killing an enemy. a higher reward for the fewer turns the enemy bot has taken (an even higher reward for killing a 'boss' bot)
  b) +3 : reward for collecting bonus points
  c) +1 : reward for increasing player power
  c) -3 : punish for collecting negative points
  d) -6 : punish for letting the enemy capture the base
  e) -6 : punish for invalid action (eg. choosing invalid direction for player; moving player that does not exist;)
  f) -3 : punish for player not moving and on the edge of the board
  g) maybe reward for moving towards enemy?
  h) maybe punish for moving away from enemy?

- What should be the inputs?
  a) 81 (9 x 9) inputs for board state with values...
     - 0 = empty square
     - 1-5 = player state (1=player not moving, 2=player moving up, 3=player moving right, 4=player moving down, 5=player moving left)
     - 6-9 = enemy state (1=enemy moving up, 2=enemy moving right, 3=enemy moving down, 4=enemy moving left)
     - 10 = increase power square
     - 11 = decrease power square

     where board states are ordered from left to right, top to bottom on the board

- What should be the outputs?
  a) 80 actions (a number in the range 0-79) : that is 4 possible actions for one of 20 possible players
     - 0-3   = player1 (up, right, down, left)
     - 4-7   = player2 (up, right, down, left)
     - 8-11  = player3 (up, right, down, left)
     - 12-15 = player4 (up, right, down, left)
     - 16-19 = player5 (up, right, down, left)
     ...
     - 76-79 = player20 (up, right, down, left)

     where players are ordered from left to right, top to bottom on the board

--------------------------------------
--- Approach to training the brain ---
--------------------------------------

Experiment #1: Train the brain to select a player unit that exists using 1 input - different ranges
---------------------------------------------------------------------------------------------------
a) Using 1 input in the range 1-20:
    - 1 input in the range 1-20 to represent the number of player units
    - 80 outputs
    - reward brain for outputting an action for a an existing player
    - punish brain for outputting an action for a non existing player
    - track reward
    - reward going consistently up indicates the brain as learned to select an existing player

    eg. For an input of 3,
        - reward +1 to the brain for an output in the range 0-11
        - reward -2 to the brain for an output in the range 12-79

b) Using 1 input in the range 1-10:
    - same as a) except with a reduced input range
    - 40 outputs

c) Using 1 input in the range 1-5:
    - same as b) except with a reduced input range
    - 20 outputs

d) Using 1 input in the range 1-2:
    - same as c) except with a reduced input range
    - 8 outputs


Experiment #2: Train the brain to select a player unit that exists using 1 input - values 1-2
---------------------------------------------------------------------------------------------
Using 1 input in the range 1-2:
    - 1 input in the range 1-2 to represent the number of player units
    - 8 outputs
    - keep training until average reward is > 0.9


Experiment #3: Train the brain to select a player unit that exists using 81 inputs - values 0-1
------------------------------------------------------------------
a) Using 81 inputs of values 0 or 1:
    - same as a) except using 81 inputs for board state with values...
         - 0 = empty square
         - 1 = player

    eg. For an input of 1,1,1,0,0,0,......0
        - reward +1 to the brain for an output in the range 0-11
        - reward -2 to the brain for an output in the range 12-79

Outcome:
- Having 81 inputs takes about 16 seconds per iterations (really slow!)
- ETA for 160,000 iterations is 106 minutes


Experiment #4: Graph Number of Inputs vs Learning Time (for 8000 iterations)
------------------------------------------------------------------
Purpose:
  Determine how the number of inputs used effects the learning time

Result
- Having 8 inputs takes about 67 mins to do 160,000 iterations
- Having 16 inputs takes about 69 mins to do 160,000 iterations


Experiment #5: Train with 9 inputs
----------------------------------
- each input represents 9 board square states which have been binary encoded into 36 bits (9 x 4 bits)

According to this article the maximum integer which can be represented in javascript is Math.pow(2, 53)
http://www.2ality.com/2012/04/number-encoding.html



Experiment #6: Train the brain to select a player unit that exists using 81 inputs - values 0-1
------------------------------------------------------------------
a) Using 81 inputs of values 0 or 1:
    - same as a) except using 81 inputs for board state with values...
         - 0 = empty square
         - 1 = player

    eg. For an input of 1,1,1,0,0,0,......0
        - reward +1 to the brain for an output in the range 0-11
        - reward -2 to the brain for an output in the range 12-79


Experiment #7: Train the brain to select a player unit that exists using 81 inputs - values 0-5
-----------------------------------------------------------------------------------------------
a) Using 81 inputs of values 0-5:
    - same as a) except using 81 inputs for board state with values...
         - 0 = empty square
         - 1-5 = player state (1=player not moving, 2=player moving up, 3=player moving right, 4=player moving down, 5=player moving left)

    eg. For an input of 1,2,3,0,0,0,......0
        - reward +1 to the brain for an output in the range 0-11
        - reward -2 to the brain for an output in the range 12-79


Experiment #8: Train the brain to select a player move that exists
------------------------------------------------------------------
    - 81 inputs for board state with values...
         - 0 = empty square
         - 1-5 = player state
             - 1 = not moving
             - 2 = moving up
             - 3 = moving right
             - 4 = moving down
             - 5 = moving left
    - 1 output in the range 0-79
      - 0-3   = player1 (up, right, down, left)
      - 4-7   = player2 (up, right, down, left)
      - 8-11  = player3 (up, right, down, left)
      - 12-79 = player4 .... player20
    - reward brain for outputting an action for a an existing player
    - reward brain for outputting an available player move
    - punish brain for outputting an action for a non existing player
    - punish brain for outputting a non-available player move
    - track reward
    - reward going consistently up indicates the brain as learned to select an existing player

    eg. For an input of 1,2,3,0,0,0,......0
        Note: This represents
            - player1: not moving
            - player2: moving up
            - player3: moving right
            - no other players
        - reward +1 to the brain for an output in the range 0-11
        - reward +1 to the brain for an output of either 0-3, 5-8, 10-11
        - reward -2 to the brain for an output in the range 12-79
        - reward -2 to the brain for an output of either 4 or 9

----------------------
*** Decisions Made ***
----------------------

Game State should be determined from javascript game internals
- For training purposes I'll need to read the game state from the game internals to improve training times
- Once the brain is fully trained and performance is not an issue, game state could be read
  from the DOM so that the AI bot can executed as a bookmarklet on the actual Math Effect website

Number of inputs to the brain
- Found that the more inputs to the brain, the longer it takes to train
- Instead of 81 inputs, reduced to 9 inputs using binary encoding


-------------
*** Tasks ***
-------------

- Alter Math Effect
    - Have 'Training Mode' - trains brain and does NOT update the UI
    - Have 'AutoPlay Mode' - uses the trained brain to play the game with normal UI updates

- Read game state from the game internals when training the brain

- Prepare video presentation


-------------------
*** Other Ideas ***
-------------------

Training a 'Better' brain
1) Train 10 brains
2) Choose the top (2 or 5) performing brains as the base for 10 new brains
3) Repeat step 1)

Measuring a trained brain's performance
- play the game using the trained brain
- measure the average score achieved for 50 games

Distributed Learning
- have multiple computers training brains
- controlled by a master computer which chooses the best performing brains as the base for new brains to be trained


------------------------
*** Ben's Top Scores ***
------------------------
game 1: 14
game 2: 30
game 3: 175
game 4: 96
game 5: 363
game 6: 125
game 7: 56
game 8: 356
game 9: 1446  <-- Top Score!


----------------------
*** Bot Name Ideas ***
----------------------
HIMEA - Highly Intelligent Math Effect Automation
MEA - Math Effect Automation


---------------------------
*** Presentation Script ***
---------------------------
Introduction
- The Goal: Create an AI bot which can play the Math Effect game better than a human

- Introducing Math Effect
    - a single player, turn based, strategy game, where the player must defend their base as long as they can.
    - every turn the enemy units will move closer to your base, which is positioned in the center of the board
    - every turn the player must make a decision: which player should I move? and in which direction?
    - every turn the 'power' of all units on the board increases by 1
    - every second turn an enemy unit is spawned on the edge of the board
    - when a player unit and an enemy unit meet, the unit with the least power is 'destroyed'

- Introducing the Human (pic of Ben Birch (aka. Beer Baron)
    Characteristics
    - input: optic nerve
    - brain type: organic
    - output: hand controls mouse to point and click

- Introducing 'ME Bot' (pic of javascript code?)
    Characteristics
    - input: binary
    - brain type: artificial (Deep Q Learning)
    - output: binary

- Other characteristics could be 'Strengths', 'Special Skill', 'Weakness' ?  (think of something humorous)


Approach Taken
- Using the javascript library ConvNetJS
- Implementation is based on the paper 'Playing Atari with Deep Reinforcement Learning' http://arxiv.org/pdf/1312.5602v1.pdf
- Show the Deep Q Learning Demo http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html

- Show diagram explaining Deep Q Learning Training technique
   - 1. inputs
   - 2. brain
   - 3. outputs
   - 4. reward
   - 5. repeat 1.

Results
- Show 'ME Bot' with trained brain playing Math Effect, displaying highest score
- Show 'Human' playing Math Effect , displaying highest score
- Who scored higher 'ME Bot' or 'Human'


Conclusion
- if 'ME Bot' wins - AI will destroy all humans! Be Afraid! ;-)
- if 'Human' wins - don't worry, AI will not defeat the human race! ;-)


--------------------------------
*** Other Presentation Ideas ***
--------------------------------

Hymie YouTube videos
- Hymie Out Of Control (first 30 secs): https://www.youtube.com/watch?v=7eR2CMgRmnM
- Hymie As New Guy (49 secs): https://www.youtube.com/watch?v=5rXHvb8gQUU

Narrated
- Using a computer generated voice and subtitles
- image of speaker in the bottom-left corner, eg. HAL or brain
- Could upload the video to YouTube to apply subtitles


--------------------
*** Observations ***
--------------------

--------------
Train Brain 01
--------------
Summary:           Brain does not appear to improve after 200,000 iterations

Number of Inputs:  81
Number of Actions: 12
Console Output:    train-brain-01-with-81-inputs-console.txt
Stats Output:      train-brain-01-with-81-inputs-stats.csv

Overall:           OK
Training Time:     18.15 hours (65,370,968 ms)
Training Runs:     2,391,898
% valid actions:   Stable at 97%
av. rewards:       Stable at 1.2
av. points:        Stable at 30

--------------
Train Brain 02
--------------
Summary:           Brain does not appear to learn when provided binary encoded inputs

Number of Inputs:  9  (Note: binary encoded 81 inputs into 9)
Number of Actions: 12
Console Output:    train-brain-02-with-9-inputs-console.txt
Stats Output:      train-brain-02-with-9-inputs-stats.csv

Overall Result:    Bad
Training Time:     1.22 hours (4,395,303 ms)
Training Runs:     165,904
% valid actions:   Decreasing
av. rewards:       Decreasing
av. points:        Stable at 6

