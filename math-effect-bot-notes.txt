----------------------------
--- Original source code ---
----------------------------
Math Effect source code on github
- https://github.com/ilfate/laravel.ilfate.net

-----------
--- DOM ---
-----------
DOM for player and one button
-----------------------------
<div class="unit-1 playerUnit">
  <div class="tdButton fa-arrow-circle-down"></div>
</div>

DOM for final modal dialog
--------------------------
<div id="myModal"
    <div id="turnsSurvived">10</div>
    <div id="unitsKilled">2</div>
    <div id="pointsEarned">16</div>

     <a class="btn">Restart</a>
</div>
	 
---------------------
--- Code Snippets ---
---------------------
Click first player button found
-------------------------------
$(".tdButton:visible").first().click()

Output final result
-------------------
console.log("[turns: " + $("#turnsSurvived").text() + "][kills: " + $("#unitsKilled").text() + "][points: " + $("#pointsEarned").text() + "]");

// [turns: 5][kills: 2][points: 3]

AutoPlay game forever
---------------------
var autoPlay = function () {
    if ($("#myModal:visible").size() == 0) {
        $(".tdButton:visible").first().click();
		setTimeout(autoPlay, 1000);
    } else {
        console.log("[turns: " + $("#turnsSurvived").text() + "][kills: " + $("#unitsKilled").text() + "][points: " + $("#pointsEarned").text() + "]");
		
		window.location = "http://ilfate.net/MathEffect"
		setTimeout(autoPlay, 3000);
	}
}

setTimeout(autoPlay, 1000);

-----------------------
--- Q-Learner Notes ---
-----------------------

ConvnetJS - Reinforcement Learning Example
------------------------------------------
http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html

Questions
- What actions should be rewarded/punished?
  a) +5 : reward for killing an enemy. a higher reward for the fewer turns the enemy bot has taken (an even higher reward for killing a 'boss' bot)
  b) +3 : reward for collecting bonus points
  c) +1 : reward for increasing player power
  c) -3 : punish for collecting negative points
  d) -6 : punish for letting the enemy capture the base
  e) -6 : punish for invalid action (eg. choosing invalid direction for player; moving player that does not exist;)
  f) -3 : punish for player not moving and on the edge of the board
  g) maybe reward for moving towards enemy?
  h) maybe punish for moving away from enemy?

- What should be the inputs?
  a) 81 (9 x 9) inputs for board state with values...
     - 0 = empty square
     - 1-5 = player state (1=player not moving, 2=player moving up, 3=player moving right, 4=player moving down, 5=player moving left)
     - 6-9 = enemy state (1=enemy moving up, 2=enemy moving right, 3=enemy moving down, 4=enemy moving left)
     - 10 = increase power square
     - 11 = decrease power square

     where board states are ordered from left to right, top to bottom on the board

- What should be the outputs?
  a) 80 actions (a number in the range 0-79) : that is 4 possible actions for one of 20 possible players
     - 0-3   = player1 (up, right, down, left)
     - 4-7   = player2 (up, right, down, left)
     - 8-11  = player3 (up, right, down, left)
     - 12-15 = player4 (up, right, down, left)
     - 16-19 = player5 (up, right, down, left)
     ...
     - 76-79 = player20 (up, right, down, left)

     where players are ordered from left to right, top to bottom on the board

--------------------------------------
--- Approach to training the brain ---
--------------------------------------

Experiment #1: Train the brain to select a player unit that exists using 1 input - different ranges
---------------------------------------------------------------------------------------------------
a) Using 1 input in the range 1-20:
    - 1 input in the range 1-20 to represent the number of player units
    - 80 outputs
    - reward brain for outputting an action for a an existing player
    - punish brain for outputting an action for a non existing player
    - track reward
    - reward going consistently up indicates the brain as learned to select an existing player

    eg. For an input of 3,
        - reward +1 to the brain for an output in the range 0-11
        - reward -2 to the brain for an output in the range 12-79

b) Using 1 input in the range 1-10:
    - same as a) except with a reduced input range
    - 40 outputs

c) Using 1 input in the range 1-5:
    - same as b) except with a reduced input range
    - 20 outputs

d) Using 1 input in the range 1-2:
    - same as c) except with a reduced input range
    - 8 outputs


Experiment #2: Train the brain to select a player unit that exists using 1 input - values 1-2
---------------------------------------------------------------------------------------------
Using 1 input in the range 1-2:
    - 1 input in the range 1-2 to represent the number of player units
    - 8 outputs
    - keep training until average reward is > 0.9


Experiment #3: Train the brain to select a player unit that exists using 81 inputs - values 0-1
------------------------------------------------------------------
a) Using 81 inputs of values 0 or 1:
    - same as a) except using 81 inputs for board state with values...
         - 0 = empty square
         - 1 = player

    eg. For an input of 1,1,1,0,0,0,......0
        - reward +1 to the brain for an output in the range 0-11
        - reward -2 to the brain for an output in the range 12-79

Outcome:
- Having 81 inputs takes about 16 seconds per iterations (really slow!)
- ETA for 160,000 iterations is 106 minutes


Experiment #4: Graph Number of Inputs vs Learning Time (for 8000 iterations)
------------------------------------------------------------------
Purpose:
  Determine how the number of inputs used effects the learning time

Result
- Having 8 inputs takes about 67 mins to do 160,000 iterations
- Having 16 inputs takes about 69 mins to do 160,000 iterations


Experiment #5: Train with 9 inputs
----------------------------------
- each input represents 9 board square states which have been binary encoded into 36 bits (9 x 4 bits)

According to this article the maximum integer which can be represented in javascript is Math.pow(2, 53)
http://www.2ality.com/2012/04/number-encoding.html



Experiment #6: Train the brain to select a player unit that exists using 81 inputs - values 0-1
------------------------------------------------------------------
a) Using 81 inputs of values 0 or 1:
    - same as a) except using 81 inputs for board state with values...
         - 0 = empty square
         - 1 = player

    eg. For an input of 1,1,1,0,0,0,......0
        - reward +1 to the brain for an output in the range 0-11
        - reward -2 to the brain for an output in the range 12-79


Experiment #7: Train the brain to select a player unit that exists using 81 inputs - values 0-5
-----------------------------------------------------------------------------------------------
a) Using 81 inputs of values 0-5:
    - same as a) except using 81 inputs for board state with values...
         - 0 = empty square
         - 1-5 = player state (1=player not moving, 2=player moving up, 3=player moving right, 4=player moving down, 5=player moving left)

    eg. For an input of 1,2,3,0,0,0,......0
        - reward +1 to the brain for an output in the range 0-11
        - reward -2 to the brain for an output in the range 12-79


Experiment #8: Train the brain to select a player move that exists
------------------------------------------------------------------
    - 81 inputs for board state with values...
         - 0 = empty square
         - 1-5 = player state
             - 1 = not moving
             - 2 = moving up
             - 3 = moving right
             - 4 = moving down
             - 5 = moving left
    - 1 output in the range 0-79
      - 0-3   = player1 (up, right, down, left)
      - 4-7   = player2 (up, right, down, left)
      - 8-11  = player3 (up, right, down, left)
      - 12-79 = player4 .... player20
    - reward brain for outputting an action for a an existing player
    - reward brain for outputting an available player move
    - punish brain for outputting an action for a non existing player
    - punish brain for outputting a non-available player move
    - track reward
    - reward going consistently up indicates the brain as learned to select an existing player

    eg. For an input of 1,2,3,0,0,0,......0
        Note: This represents
            - player1: not moving
            - player2: moving up
            - player3: moving right
            - no other players
        - reward +1 to the brain for an output in the range 0-11
        - reward +1 to the brain for an output of either 0-3, 5-8, 10-11
        - reward -2 to the brain for an output in the range 12-79
        - reward -2 to the brain for an output of either 4 or 9

----------------------
*** Decisions Made ***
----------------------

Game State should be determined from javascript game internals
- For training purposes I'll need to read the game state from the game internals to improve training times
- Once the brain is fully trained and performance is not an issue, game state could be read
  from the DOM so that the AI bot can executed as a bookmarklet on the actual Math Effect website

Number of inputs to the brain
- Found that the more inputs to the brain, the longer it takes to train
- Instead of 81 inputs, reduced to 9 inputs using binary encoding


-------------
*** Tasks ***
-------------

- Alter Math Effect
    - Have 'Training Mode' - trains brain and does NOT update the UI
    - Have 'AutoPlay Mode' - uses the trained brain to play the game with normal UI updates

- Read game state from the game internals when training the brain

- Prepare video presentation


-------------------
*** Other Ideas ***
-------------------

Training a 'Better' brain
1) Train 10 brains
2) Choose the top (2 or 5) performing brains as the base for 10 new brains
3) Repeat step 1)

Measuring a trained brain's performance
- play the game using the trained brain
- measure the average score achieved for 50 games

Distributed Learning
- have multiple computers training brains
- controlled by a master computer which chooses the best performing brains as the base for new brains to be trained


------------------------
*** Ben's Top Scores ***
------------------------
game 1: 14
game 2: 30
game 3: 175
game 4: 96
game 5: 363
game 6: 125
game 7: 56
game 8: 356
game 9: 1446  <-- Top Score!


----------------------
*** Bot Name Ideas ***
----------------------
HIMEA - Highly Intelligent Math Effect Automation
MEA - Math Effect Automation


---------------------------
*** Presentation Script ***
---------------------------
Introduction
- The Goal: Create an AI bot which can play the Math Effect game better than a human

- Introducing Math Effect
    - a single player, turn based, strategy game, where the player must defend their base as long as they can.
    - every turn the enemy units will move closer to your base, which is positioned in the center of the board
    - every turn the player must make a decision: which player should I move? and in which direction?
    - every turn the 'power' of all units on the board increases by 1
    - every second turn an enemy unit is spawned on the edge of the board
    - when a player unit and an enemy unit meet, the unit with the least power is 'destroyed'

- Introducing the Human (pic of Ben Birch (aka. Beer Baron)
    Characteristics
    - input: optic nerve
    - brain type: organic
    - output: hand controls mouse to point and click

- Introducing 'ME Bot' (pic of javascript code?)
    Characteristics
    - input: binary
    - brain type: artificial (Deep Q Learning)
    - output: binary

- Other characteristics could be 'Strengths', 'Special Skill', 'Weakness' ?  (think of something humorous)


Approach Taken
- Using the javascript library ConvNetJS
- Implementation is based on the paper 'Playing Atari with Deep Reinforcement Learning' http://arxiv.org/pdf/1312.5602v1.pdf
- Show the Deep Q Learning Demo http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html

- Show diagram explaining Deep Q Learning Training technique
   - 1. inputs
   - 2. brain
   - 3. outputs
   - 4. reward
   - 5. repeat 1.

Results
- Show 'ME Bot' with trained brain playing Math Effect, displaying highest score
- Show 'Human' playing Math Effect , displaying highest score
- Who scored higher 'ME Bot' or 'Human'


Conclusion
- if 'ME Bot' wins - AI will destroy all humans! Be Afraid! ;-)
- if 'Human' wins - don't worry, AI will not defeat the human race! ;-)


--------------------------------
*** Other Presentation Ideas ***
--------------------------------

Hymie YouTube videos
- Hymie Out Of Control (first 30 secs): https://www.youtube.com/watch?v=7eR2CMgRmnM
- Hymie As New Guy (49 secs): https://www.youtube.com/watch?v=5rXHvb8gQUU

Narrated
- Using a computer generated voice and subtitles
- image of speaker in the bottom-left corner, eg. HAL or brain
- Could upload the video to YouTube to apply subtitles


--------------------
*** Observations ***
--------------------

--------------
Train Brain 01
--------------
Summary:           Autoplay best Game:  [*** turns: 26, kills: 9, points: 113 ***]
                   Average points per game: 30
                   Brain does not appear to improve after 200,000 iterations.
                   Makes moves which spawn more than 3 player units. Note: Brain can only control 3 player units)
                   Player units appear to be stuck on the edge of the board quite a bit.

Number of Inputs:  81
Number of Actions: 12  (represents 3 player units)
Rewards:
    +5: Killed an enemy
    +1: Regular move
    -6: Invalid move
            - moving player that does not exist
            - moving player in direction they are already moving
            - moving player off the board
    -6: Letting enemy capture the base

Console Output:    train-brain-01-with-81-inputs-console.txt
Stats Output:      train-brain-01-with-81-inputs-stats.csv

Overall:           OK
Training Time:     18.15 hours (65,370,968 ms)
Training Runs:     2,391,898
% valid actions:   Stable at 97%
av. rewards:       Stable at 1.2
av. points:        Stable at 30


---------------------------------------
Train Brain 02: Binary Encoded Input
---------------------------------------
Summary:           Brain does not appear to learn when provided binary encoded inputs

Number of Inputs:  9  (Note: binary encoded 81 inputs into 9)
Number of Actions: 12
Console Output:    train-brain-02-with-9-inputs-console.txt
Stats Output:      train-brain-02-with-9-inputs-stats.csv

Overall Result:    Fail!  Did not learn
Training Time:     1.22 hours (4,395,303 ms)
Training Runs:     165,904
% valid actions:   Decreasing
av. rewards:       Decreasing
av. points:        Stable at 6


------------------------------
Train Brain 03 a) Unoptimized
------------------------------
Summary:           Added more rewards/punishments.
                   Punish spawning more than 3 player units & reward when player units reduces to 3
                   Introduced the concept of the 'base zone' to encourage player units to stay near the base.
                   ** VERY SLOW **

Number of Inputs:  81
Number of Actions: 12  (represents 3 player units)
Rewards:
    +5: Killed an enemy
    +5: Reduced number of player units when more than three player units
    +5: Moved player unit inside the 'base zone'
    +1: Regular move
    -6: Invalid move
            - moved player that does not exist
            - moved player in direction they are already moving
            - moved player off the board
    -6: Let enemy capture the base
    -6: Spawned a unit resulting in more than three player units
    -6: Moved player unit outside the 'base zone'

Console Output:    train-brain-03-with-81-inputs-console.txt
Stats Output:      train-brain-03-with-81-inputs-stats.csv

Overall:            Fail!  Too Slow!
Training Time:      13.5 hours (48833427 ms)
Training Runs:      230,710
% valid actions:    40%
av. rewards:        1.5
av. points:         19


------------------------------
Train Brain 03 b) Optimized
------------------------------
Summary:            Same as a) but with an optimization of the check for within the 'base zone', using a lookup table

Overall:
Training Time:      25 hours (91262649 ms)
Training Runs:      7,171,464
% valid actions:    96%
av. rewards:        1.8
av. points:         41
Top Score:          360


---------------------------------------
Train Brain 04: Larger punishments
---------------------------------------
Summary:            Increased punishments for 'Invalid move' and 'Let enemy capture the base' from -6 to -100
                    Average points did not increase at all over time

Overall:            Fail!  Average Points did not increase at all
Training Time:      6 hours (21669069 ms)
Training Runs:      695,147
% valid actions:    93%
av. rewards:        -19
av. points:         7


-----------------------------------------------------------------------------------------
Train Brain 05a: Changed logic for applying the 'Let enemy capture the base' punishment
-----------------------------------------------------------------------------------------
Summary:            Changed logic for applying the 'Let enemy capture the base' punishment.
                    Only apply this punishment if the player moves off the base without defending
                    Shows newly trained behaviour of spawning less units and staying in the 'base zone'

Overall:            OK!
Training Time:      13 hours (48026020 ms)
Training Runs:      2,245,485
% valid actions:    96%
av. rewards:        1.1
av. points:         23


-----------------------------------------------------------------------------------------
Train Brain 05b: Continue training of 05a
-----------------------------------------------------------------------------------------
Summary:            Continue training of 05a

Overall:
Training Time:      23 hours  (or another 10 hours (35824548 ms) from 05a)
Training Runs:      4,017,002
% valid actions:    96%
av. rewards:        1.2
av. points:         25


-----------------------------------------------------------------------------------------
Train Brain 6: Tweaked rewards
-----------------------------------------------------------------------------------------
Summary:            Tweaked rewards
                    - increased 'Killed an enemy' reward from +5 to +10
                    - decreased 'Let enemy capture the base' reward from -6 to -11

Rewards:
-> +10: (increased from +5) Killed an enemy
    +5: Reduced number of player units when more than three player units
    +5: Moved player unit inside the 'base zone'
    +1: Regular move
    -6: Invalid move
            - moved player that does not exist
            - moved player in direction they are already moving
            - moved player off the board
-> -11: (Decreased from -6) Let enemy capture the base
    -6: Spawned a unit resulting in more than three player units
    -6: Moved player unit outside the 'base zone'

Overall:
Training Time:      9.7 hours (35059365 ms)
Training Runs:      338,884
% valid actions:    95%
av. rewards:        1.4
av. points:         20


-----------------------------------------------------------------------------------------
Train Brain 7a: Increased Number of Actions
-----------------------------------------------------------------------------------------
Summary:          Increased Number of Actions from 12 to 64 to support up to 16 player units
                  Reverted previous reward tweaks to improve training performance
                    - decreased 'Killed an enemy' reward from +10 to +5
                    - increased 'Let enemy capture the base' reward from -11 to -10

Rewards:
    +5: Killed an enemy
    +5: Reduced number of player units when more than three player units
    +5: Moved player unit inside the 'base zone'
    +1: Regular move
    -6: Invalid move
            - moved player that does not exist
            - moved player in direction they are already moving
            - moved player off the board
    -6: Let enemy capture the base
    -6: Spawned a unit resulting in more than three player units
    -6: Moved player unit outside the 'base zone'

Overall:            OK - but rewards decreasing at end
Training Time:      14.3 hours (51563488 ms)
Training Runs:      614,901
% valid actions:    90%
av. rewards:        0.54
av. points:         19


-----------------------------------------------------------------------------------------
Train Brain 7b: Continuing...
-----------------------------------------------------------------------------------------
Summary:            Continuing...

Overall:            OK - av. rewards and av. points increased slightly
Training Time:      20.9 hours (75357715 ms)
Training Runs:      1,703,901
% valid actions:    93%
av. rewards:        0.96
av. points:         23


-----------------------------------------------------------------------------------------
Train Brain 7c: Continuing...
-----------------------------------------------------------------------------------------
Summary:            Continuing...

Overall:
Training Time:
Training Runs:
% valid actions:
av. rewards:
av. points:


-----------------------------------------------------------------------------------------
Train Brain 8: Deterministic Game & setting random_action_distribution on brain
-----------------------------------------------------------------------------------------
Summary:            Changed game to be deterministic using a seeded random engine
                    Set random_action_distribution for brain to reduce invalid moves

Overall:            Fail - did not train!
Training Time:      16.8 hours (60460883.89399998 ms)
Training Runs:      225,852
% valid actions:    6%
av. rewards:        -5.6
av. points:         4.5


-----------------------------------------------------------------------------------------
Train Brain 9: Deterministic Game
-----------------------------------------------------------------------------------------
Summary:            Changed game to be deterministic using a seeded random engine
                    Reverted previous change to set random_action_distribution for brain to reduce invalid moves

Overall:
Training Time:
Training Runs:
% valid actions:
av. rewards:
av. points:

